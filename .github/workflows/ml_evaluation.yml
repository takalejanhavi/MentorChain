name: ML Model Evaluation

on:
  pull_request:
    branches: [ main ]
    paths:
      - 'python-service/**'
      - 'docs/psychometrics/**'
  push:
    branches: [ main ]
    paths:
      - 'python-service/**'

jobs:
  model-evaluation:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('python-service/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        cd python-service
        pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Run unit tests
      run: |
        cd python-service
        python -m pytest ../tests/inference.test.py -v --cov=inference --cov-report=xml
    
    - name: Train and evaluate model
      run: |
        cd python-service
        python train_production_model.py
      env:
        PYTHONPATH: ${{ github.workspace }}/python-service
    
    - name: Validate model performance
      run: |
        cd python-service
        python -c "
        import json
        import sys
        
        # Load model metadata
        try:
            with open('models/model_metadata_latest.json', 'r') as f:
                metadata = json.load(f)
            
            cv_accuracy = metadata['performance_metrics']['cv_accuracy']
            test_accuracy = metadata['performance_metrics']['test_accuracy']
            top3_accuracy = metadata['performance_metrics']['top3_accuracy']
            
            print(f'CV Accuracy: {cv_accuracy:.4f}')
            print(f'Test Accuracy: {test_accuracy:.4f}')
            print(f'Top-3 Accuracy: {top3_accuracy:.4f}')
            
            # Quality gates
            if cv_accuracy < 0.88:
                print(f'❌ CV accuracy {cv_accuracy:.4f} below threshold 0.88')
                sys.exit(1)
            
            if test_accuracy < 0.85:
                print(f'❌ Test accuracy {test_accuracy:.4f} below threshold 0.85')
                sys.exit(1)
            
            if top3_accuracy < 0.95:
                print(f'❌ Top-3 accuracy {top3_accuracy:.4f} below threshold 0.95')
                sys.exit(1)
            
            print('✅ All quality gates passed!')
            
        except FileNotFoundError:
            print('❌ Model metadata not found')
            sys.exit(1)
        except Exception as e:
            print(f'❌ Error validating model: {e}')
            sys.exit(1)
        "
    
    - name: Upload model artifacts
      uses: actions/upload-artifact@v3
      if: success()
      with:
        name: model-artifacts
        path: |
          python-service/models/
          python-service/student_training_data_v2.csv
        retention-days: 30
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      if: success()
      with:
        file: python-service/coverage.xml
        flags: python-service
        name: python-service-coverage

  integration-test:
    runs-on: ubuntu-latest
    needs: model-evaluation
    
    services:
      mongodb:
        image: mongo:5.0
        ports:
          - 27017:27017
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install backend dependencies
      run: |
        cd backend
        npm install
    
    - name: Install Python service dependencies
      run: |
        cd python-service
        pip install -r requirements.txt
    
    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        name: model-artifacts
        path: python-service/
    
    - name: Start Python service
      run: |
        cd python-service
        python app.py &
        sleep 10
      env:
        PORT: 5002
    
    - name: Run integration tests
      run: |
        cd backend
        npm test -- --testPathPattern=assessment
      env:
        NODE_ENV: test
        MONGODB_TEST_URI: mongodb://localhost:27017/careerguide_test
        MODEL_API_URL: http://localhost:5002/api
        JWT_SECRET: test_secret_key_for_ci
    
    - name: Test API endpoints
      run: |
        # Test health endpoint
        curl -f http://localhost:5002/api/health || exit 1
        
        # Test model info endpoint
        curl -f http://localhost:5002/api/model/info || exit 1
        
        echo "✅ API endpoints responding correctly"

  performance-benchmark:
    runs-on: ubuntu-latest
    needs: model-evaluation
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        cd python-service
        pip install -r requirements.txt
        pip install memory-profiler
    
    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        name: model-artifacts
        path: python-service/
    
    - name: Run performance benchmark
      run: |
        cd python-service
        python -c "
        import time
        import psutil
        import numpy as np
        from inference import predict_career
        
        # Sample input
        sample_input = {
            'trait_scores': {
                'big_five': {
                    'Openness': 8.5, 'Conscientiousness': 7.2, 'Extraversion': 6.1,
                    'Agreeableness': 7.8, 'Neuroticism': 3.4
                },
                'riasec': {
                    'Realistic': 4.2, 'Investigative': 8.7, 'Artistic': 6.3,
                    'Social': 7.1, 'Enterprising': 5.9, 'Conventional': 6.8
                }
            }
        }
        
        # Warmup
        for _ in range(5):
            predict_career(sample_input)
        
        # Benchmark
        times = []
        for _ in range(100):
            start = time.time()
            result = predict_career(sample_input)
            end = time.time()
            times.append(end - start)
        
        avg_time = np.mean(times)
        p95_time = np.percentile(times, 95)
        
        print(f'Average inference time: {avg_time*1000:.2f}ms')
        print(f'95th percentile: {p95_time*1000:.2f}ms')
        
        # Performance thresholds
        if avg_time > 0.5:  # 500ms threshold
            print(f'❌ Average inference time {avg_time*1000:.2f}ms exceeds 500ms threshold')
            exit(1)
        
        if p95_time > 1.0:  # 1s threshold
            print(f'❌ 95th percentile {p95_time*1000:.2f}ms exceeds 1000ms threshold')
            exit(1)
        
        print('✅ Performance benchmarks passed!')
        "